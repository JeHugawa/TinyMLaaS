{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39900b1b",
   "metadata": {},
   "source": [
    "# Architecture Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab010eb3",
   "metadata": {},
   "source": [
    "This page contains general information about the applications architecture and how different parts of the application work and communicate with each other.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "- ML model training \n",
    "- Data storage & loading\n",
    "- ML model quantization and optimization\n",
    "- ML model compilation\n",
    "- Firmware packaging\n",
    "\n",
    "## Enabling Technologies\n",
    "\n",
    "- Tensorflow;\n",
    "- Docker\n",
    "- Streamlit\n",
    "- Aws S3\n",
    "\n",
    "## What Has Been Implemented?\n",
    "\n",
    "![General flow of the service](images/asd.png)\n",
    "\n",
    "- Right now uploading the firmware is supported via uploading the firmware to Dockerhub and notifying the Bridging server to pull the docker image and upload the firmware via Serial on to the device.\n",
    "- TODO regarding the above picture containing all the elements of the application is the ability to update Over-The-Air for Wifi-enabled devices.\n",
    "\n",
    "### Implemented Features\n",
    "\n",
    "- ML model training\n",
    "- Data storage to LocalStack/S3\n",
    "- ML model optimization and compression\n",
    "- Supported devices Arduino Nano 33 BLE Sense and RPI pico\n",
    "- Querying prediction results from the end-device\n",
    "- Refer to the actual application and the above picture for more information.\n",
    "\n",
    "## Neural Network Architecture\n",
    "\n",
    "![Current NN Architecture](images/model.tflite.png)\n",
    "\n",
    "When training a Neural Network for TinyML, there are a few things to keep in mind to ease the process. The resulting model must fit into the device.\n",
    "\n",
    "Things that affect the resulting models size include:\n",
    "    \n",
    "- Amount of layers used in the model. Fewer layers usually results in smaller sized models.\n",
    "- Used OPs, e.g., Convolution layers can take up quite a bit of space.\n",
    "- Quantization is almost necessary for the model to compressed into a usable size.\n",
    "\n",
    "TinyML related tips\n",
    "\n",
    "- OPs used in the keras model training must also be supported in tflite-micro.\n",
    "- [Netron](https://netron.app), Use netron to visualize models and check that you have all the OPs enabled in your Firmware. Every OP must be declared in order for the model to work.\n",
    "\n",
    "Misc.\n",
    "\n",
    "- [Tensorflow](https://www.tensorflow.org/tutorials/images/classification) has extensive amount of information and tutorials regarding almost all necessary things when it comes to the application's ML related stuff.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54759a79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
